{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae687c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.connect_to_db import db_connect\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a706e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_engine = db_connect(\"corpus2_db\")\n",
    "\n",
    "batch_size = 1000\n",
    "offset = 0\n",
    "raw_df = pd.DataFrame()\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da31bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Count : 1000\n",
      "Records Count : 2000\n",
      "Records Count : 3000\n",
      "Records Count : 4000\n",
      "Records Count : 5000\n",
      "Records Count : 6000\n",
      "Records Count : 7000\n",
      "Records Count : 8000\n",
      "Records Count : 9000\n",
      "Records Count : 10000\n",
      "Records Count : 11000\n",
      "Records Count : 12000\n",
      "Records Count : 13000\n",
      "Records Count : 14000\n",
      "Records Count : 15000\n",
      "Records Count : 16000\n",
      "Records Count : 17000\n",
      "Records Count : 18000\n",
      "Records Count : 19000\n",
      "Records Count : 20000\n",
      "Records Count : 21000\n",
      "Records Count : 22000\n",
      "Records Count : 23000\n",
      "Records Count : 24000\n",
      "Records Count : 25000\n",
      "Records Count : 26000\n",
      "Records Count : 27000\n",
      "Records Count : 28000\n",
      "Records Count : 29000\n",
      "Records Count : 30000\n",
      "Records Count : 31000\n",
      "Records Count : 32000\n",
      "Records Count : 33000\n",
      "Records Count : 34000\n",
      "Records Count : 35000\n",
      "Records Count : 36000\n",
      "Records Count : 37000\n",
      "Records Count : 38000\n",
      "Records Count : 39000\n",
      "Records Count : 40000\n",
      "Records Count : 41000\n",
      "Records Count : 42000\n",
      "Records Count : 43000\n",
      "Records Count : 44000\n",
      "Records Count : 45000\n",
      "Records Count : 46000\n",
      "Records Count : 47000\n",
      "Records Count : 48000\n",
      "Records Count : 49000\n",
      "Records Count : 50000\n",
      "Records Count : 51000\n",
      "Records Count : 52000\n",
      "Records Count : 53000\n",
      "Records Count : 54000\n",
      "Records Count : 55000\n",
      "Records Count : 56000\n",
      "Records Count : 57000\n",
      "Records Count : 58000\n",
      "Records Count : 59000\n",
      "Records Count : 60000\n",
      "Records Count : 61000\n",
      "Records Count : 62000\n",
      "Records Count : 63000\n",
      "Records Count : 64000\n",
      "Records Count : 65000\n",
      "Records Count : 66000\n",
      "Records Count : 67000\n",
      "Records Count : 68000\n",
      "Records Count : 69000\n",
      "Records Count : 70000\n",
      "Records Count : 71000\n",
      "Records Count : 72000\n",
      "Records Count : 73000\n",
      "Records Count : 74000\n",
      "Records Count : 75000\n",
      "Records Count : 76000\n",
      "Records Count : 77000\n",
      "Records Count : 78000\n",
      "Records Count : 79000\n",
      "Records Count : 80000\n",
      "Records Count : 81000\n",
      "Records Count : 82000\n",
      "Records Count : 83000\n",
      "Records Count : 84000\n",
      "Records Count : 85000\n",
      "Records Count : 86000\n",
      "Records Count : 87000\n",
      "Records Count : 88000\n",
      "Records Count : 89000\n",
      "Records Count : 90000\n",
      "Records Count : 91000\n",
      "Records Count : 92000\n",
      "Records Count : 93000\n",
      "Records Count : 94000\n",
      "Records Count : 95000\n",
      "Records Count : 96000\n",
      "Records Count : 97000\n",
      "Records Count : 98000\n",
      "Records Count : 99000\n",
      "Records Count : 100000\n",
      "Records Count : 101000\n",
      "Records Count : 102000\n",
      "Records Count : 103000\n",
      "Records Count : 104000\n",
      "Records Count : 105000\n",
      "Records Count : 106000\n",
      "Records Count : 107000\n",
      "Records Count : 108000\n",
      "Records Count : 109000\n",
      "Records Count : 110000\n",
      "Records Count : 111000\n",
      "Records Count : 112000\n",
      "Records Count : 113000\n",
      "Records Count : 114000\n",
      "Records Count : 115000\n",
      "Records Count : 116000\n",
      "Records Count : 117000\n",
      "Records Count : 118000\n",
      "Records Count : 119000\n",
      "Records Count : 120000\n",
      "Records Count : 121000\n",
      "Records Count : 122000\n",
      "Records Count : 123000\n",
      "Records Count : 124000\n",
      "Records Count : 125000\n",
      "Records Count : 126000\n",
      "Records Count : 127000\n",
      "Records Count : 128000\n",
      "Records Count : 129000\n",
      "Records Count : 130000\n",
      "Records Count : 131000\n",
      "Records Count : 132000\n",
      "Records Count : 133000\n",
      "Records Count : 134000\n",
      "Records Count : 135000\n",
      "Records Count : 136000\n",
      "Records Count : 137000\n",
      "Records Count : 138000\n",
      "Records Count : 139000\n",
      "Records Count : 140000\n",
      "Records Count : 141000\n",
      "Records Count : 142000\n",
      "Records Count : 143000\n",
      "Records Count : 144000\n",
      "Records Count : 145000\n",
      "Records Count : 146000\n",
      "Records Count : 147000\n",
      "Records Count : 148000\n",
      "Records Count : 149000\n",
      "Records Count : 150000\n",
      "Records Count : 151000\n",
      "Records Count : 152000\n",
      "Records Count : 153000\n",
      "Records Count : 154000\n",
      "Records Count : 155000\n",
      "Records Count : 156000\n",
      "Records Count : 157000\n",
      "Records Count : 158000\n",
      "Records Count : 159000\n",
      "Records Count : 160000\n",
      "Records Count : 161000\n",
      "Records Count : 162000\n",
      "Records Count : 163000\n",
      "Records Count : 164000\n",
      "Records Count : 165000\n",
      "Records Count : 166000\n",
      "Records Count : 167000\n",
      "Records Count : 168000\n",
      "Records Count : 169000\n",
      "Records Count : 170000\n",
      "Records Count : 171000\n",
      "Records Count : 172000\n",
      "Records Count : 173000\n",
      "Records Count : 174000\n",
      "Records Count : 175000\n",
      "Records Count : 176000\n",
      "Records Count : 177000\n",
      "Records Count : 178000\n",
      "Records Count : 179000\n",
      "Records Count : 180000\n",
      "Records Count : 181000\n",
      "Records Count : 182000\n",
      "Records Count : 183000\n",
      "Records Count : 184000\n",
      "Records Count : 185000\n",
      "Records Count : 186000\n",
      "Records Count : 187000\n",
      "Records Count : 188000\n",
      "Records Count : 189000\n",
      "Records Count : 190000\n",
      "Records Count : 191000\n",
      "Records Count : 192000\n",
      "Records Count : 193000\n",
      "Records Count : 194000\n",
      "Records Count : 195000\n",
      "Records Count : 196000\n",
      "Records Count : 197000\n",
      "Records Count : 198000\n",
      "Records Count : 199000\n",
      "Records Count : 200000\n",
      "Records Count : 201000\n",
      "Records Count : 202000\n",
      "Records Count : 203000\n",
      "Records Count : 204000\n",
      "Records Count : 205000\n",
      "Records Count : 206000\n",
      "Records Count : 207000\n",
      "Records Count : 208000\n",
      "Records Count : 209000\n",
      "Records Count : 210000\n",
      "Records Count : 211000\n",
      "Records Count : 212000\n",
      "Records Count : 213000\n",
      "Records Count : 214000\n",
      "Records Count : 215000\n",
      "Records Count : 216000\n",
      "Records Count : 217000\n",
      "Records Count : 218000\n",
      "Records Count : 219000\n",
      "Records Count : 220000\n",
      "Records Count : 221000\n",
      "Records Count : 222000\n",
      "Records Count : 223000\n",
      "Records Count : 224000\n",
      "Records Count : 225000\n",
      "Records Count : 226000\n",
      "Records Count : 227000\n",
      "Records Count : 228000\n",
      "Records Count : 229000\n",
      "Records Count : 230000\n",
      "Records Count : 231000\n",
      "Records Count : 232000\n",
      "Records Count : 233000\n",
      "Records Count : 234000\n",
      "Records Count : 235000\n",
      "Records Count : 236000\n",
      "Records Count : 237000\n",
      "Records Count : 238000\n",
      "Records Count : 239000\n",
      "Records Count : 240000\n",
      "Records Count : 241000\n",
      "Records Count : 242000\n",
      "Records Count : 243000\n",
      "Records Count : 244000\n",
      "Records Count : 245000\n",
      "Records Count : 246000\n",
      "Records Count : 247000\n",
      "Records Count : 248000\n",
      "Records Count : 249000\n",
      "Records Count : 250000\n",
      "Records Count : 251000\n",
      "Records Count : 252000\n",
      "Records Count : 253000\n",
      "Records Count : 254000\n",
      "Records Count : 255000\n",
      "Records Count : 256000\n",
      "Records Count : 257000\n",
      "Records Count : 258000\n",
      "Records Count : 259000\n",
      "Records Count : 260000\n",
      "Records Count : 261000\n",
      "Records Count : 262000\n",
      "Records Count : 263000\n",
      "Records Count : 264000\n",
      "Records Count : 265000\n",
      "Records Count : 266000\n",
      "Records Count : 267000\n",
      "Records Count : 268000\n",
      "Records Count : 269000\n",
      "Records Count : 270000\n",
      "Records Count : 271000\n",
      "Records Count : 272000\n",
      "Records Count : 273000\n",
      "Records Count : 274000\n",
      "Records Count : 275000\n",
      "Records Count : 276000\n",
      "Records Count : 277000\n",
      "Records Count : 278000\n",
      "Records Count : 279000\n",
      "Records Count : 280000\n",
      "Records Count : 281000\n",
      "Records Count : 282000\n",
      "Records Count : 283000\n",
      "Records Count : 284000\n",
      "Records Count : 285000\n",
      "Records Count : 286000\n",
      "Records Count : 287000\n",
      "Records Count : 288000\n",
      "Records Count : 289000\n",
      "Records Count : 290000\n",
      "Records Count : 291000\n",
      "Records Count : 292000\n",
      "Records Count : 293000\n",
      "Records Count : 294000\n",
      "Records Count : 295000\n",
      "Records Count : 296000\n",
      "Records Count : 297000\n",
      "Records Count : 298000\n",
      "Records Count : 299000\n",
      "Records Count : 300000\n",
      "Records Count : 301000\n",
      "Records Count : 302000\n",
      "Records Count : 303000\n",
      "Records Count : 304000\n",
      "Records Count : 305000\n",
      "Records Count : 306000\n",
      "Records Count : 307000\n",
      "Records Count : 308000\n",
      "Records Count : 309000\n",
      "Records Count : 310000\n",
      "Records Count : 311000\n",
      "Records Count : 312000\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = f\"\"\" SELECT resource_id, curriculum_json FROM curriculum_data_tmp_phrase where curriculum_json is not null \n",
    "    LIMIT {batch_size} OFFSET {offset}\"\"\"\n",
    "    batch_df = pd.read_sql(query, con=resume_engine)\n",
    "\n",
    "    if batch_df.empty:\n",
    "        break\n",
    "\n",
    "    offset += batch_size\n",
    "    raw_df = pd.concat([raw_df, batch_df], ignore_index=True)\n",
    "    count += 1\n",
    "    print(f\"Records Count : {count * batch_size}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7317fac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311085"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17ab6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.to_csv(\"curriculum_data_tmp_phrase.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "008a5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "offset = 0\n",
    "raw_df_2 = pd.DataFrame()\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633c23e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Count : 1000\n",
      "Records Count : 2000\n",
      "Records Count : 3000\n",
      "Records Count : 4000\n",
      "Records Count : 5000\n",
      "Records Count : 6000\n",
      "Records Count : 7000\n",
      "Records Count : 8000\n",
      "Records Count : 9000\n",
      "Records Count : 10000\n",
      "Records Count : 11000\n",
      "Records Count : 12000\n",
      "Records Count : 13000\n",
      "Records Count : 14000\n",
      "Records Count : 15000\n",
      "Records Count : 16000\n",
      "Records Count : 17000\n",
      "Records Count : 18000\n",
      "Records Count : 19000\n",
      "Records Count : 20000\n",
      "Records Count : 21000\n",
      "Records Count : 22000\n",
      "Records Count : 23000\n",
      "Records Count : 24000\n",
      "Records Count : 25000\n",
      "Records Count : 26000\n",
      "Records Count : 27000\n",
      "Records Count : 28000\n",
      "Records Count : 29000\n",
      "Records Count : 30000\n",
      "Records Count : 31000\n",
      "Records Count : 32000\n",
      "Records Count : 33000\n",
      "Records Count : 34000\n",
      "Records Count : 35000\n",
      "Records Count : 36000\n",
      "Records Count : 37000\n",
      "Records Count : 38000\n",
      "Records Count : 39000\n",
      "Records Count : 40000\n",
      "Records Count : 41000\n",
      "Records Count : 42000\n",
      "Records Count : 43000\n",
      "Records Count : 44000\n",
      "Records Count : 45000\n",
      "Records Count : 46000\n",
      "Records Count : 47000\n",
      "Records Count : 48000\n",
      "Records Count : 49000\n",
      "Records Count : 50000\n",
      "Records Count : 51000\n",
      "Records Count : 52000\n",
      "Records Count : 53000\n",
      "Records Count : 54000\n",
      "Records Count : 55000\n",
      "Records Count : 56000\n",
      "Records Count : 57000\n",
      "Records Count : 58000\n",
      "Records Count : 59000\n",
      "Records Count : 60000\n",
      "Records Count : 61000\n",
      "Records Count : 62000\n",
      "Records Count : 63000\n",
      "Records Count : 64000\n",
      "Records Count : 65000\n",
      "Records Count : 66000\n",
      "Records Count : 67000\n",
      "Records Count : 68000\n",
      "Records Count : 69000\n",
      "Records Count : 70000\n",
      "Records Count : 71000\n",
      "Records Count : 72000\n",
      "Records Count : 73000\n",
      "Records Count : 74000\n",
      "Records Count : 75000\n",
      "Records Count : 76000\n",
      "Records Count : 77000\n",
      "Records Count : 78000\n",
      "Records Count : 79000\n",
      "Records Count : 80000\n",
      "Records Count : 81000\n",
      "Records Count : 82000\n",
      "Records Count : 83000\n",
      "Records Count : 84000\n",
      "Records Count : 85000\n",
      "Records Count : 86000\n",
      "Records Count : 87000\n",
      "Records Count : 88000\n",
      "Records Count : 89000\n",
      "Records Count : 90000\n",
      "Records Count : 91000\n",
      "Records Count : 92000\n",
      "Records Count : 93000\n",
      "Records Count : 94000\n",
      "Records Count : 95000\n",
      "Records Count : 96000\n",
      "Records Count : 97000\n",
      "Records Count : 98000\n",
      "Records Count : 99000\n",
      "Records Count : 100000\n",
      "Records Count : 101000\n",
      "Records Count : 102000\n",
      "Records Count : 103000\n",
      "Records Count : 104000\n",
      "Records Count : 105000\n",
      "Records Count : 106000\n",
      "Records Count : 107000\n",
      "Records Count : 108000\n",
      "Records Count : 109000\n",
      "Records Count : 110000\n",
      "Records Count : 111000\n",
      "Records Count : 112000\n",
      "Records Count : 113000\n",
      "Records Count : 114000\n",
      "Records Count : 115000\n",
      "Records Count : 116000\n",
      "Records Count : 117000\n",
      "Records Count : 118000\n",
      "Records Count : 119000\n",
      "Records Count : 120000\n",
      "Records Count : 121000\n",
      "Records Count : 122000\n",
      "Records Count : 123000\n",
      "Records Count : 124000\n",
      "Records Count : 125000\n",
      "Records Count : 126000\n",
      "Records Count : 127000\n",
      "Records Count : 128000\n",
      "Records Count : 129000\n",
      "Records Count : 130000\n",
      "Records Count : 131000\n",
      "Records Count : 132000\n",
      "Records Count : 133000\n",
      "Records Count : 134000\n",
      "Records Count : 135000\n",
      "Records Count : 136000\n",
      "Records Count : 137000\n",
      "Records Count : 138000\n",
      "Records Count : 139000\n",
      "Records Count : 140000\n",
      "Records Count : 141000\n",
      "Records Count : 142000\n",
      "Records Count : 143000\n",
      "Records Count : 144000\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query_2 = f\"\"\" SELECT resource_id, curriculum_json FROM curriculum_data where curriculum_json is not null \n",
    "    LIMIT {batch_size} OFFSET {offset}\"\"\"\n",
    "    batch_df = pd.read_sql(query_2, con=resume_engine)\n",
    "\n",
    "    if batch_df.empty:\n",
    "        break\n",
    "\n",
    "    offset += batch_size\n",
    "    raw_df_2 = pd.concat([raw_df_2, batch_df], ignore_index=True)\n",
    "    count += 1\n",
    "    print(f\"Records Count : {count * batch_size}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e894a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_id_list = raw_df['resource_id'].tolist()\n",
    "raw_df_2_filtered = raw_df_2[~raw_df_2['resource_id'].isin(resource_id_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc8b7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_raw_df = pd.concat([raw_df, raw_df_2_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6f03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_text_and_languages(text):\n",
    "    \"\"\"\n",
    "    Extract raw text and resume probability from a JSON string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        raw_text = data.get('rawText', '').strip()\n",
    "        languages = data.get('languages', '')\n",
    "        return raw_text, languages\n",
    "    except:\n",
    "        return \"raw text not present\", []\n",
    "    \n",
    "\n",
    "def process_batch(batch_df):\n",
    "    \"\"\"\n",
    "    Process a single batch of data to extract raw text.\n",
    "    \"\"\"\n",
    "    batch_df[['raw_text', 'languages']] = batch_df['curriculum_json'].apply(\n",
    "        lambda x: pd.Series(get_raw_text_and_languages(x))\n",
    "    )\n",
    "\n",
    "    no_raw_text_parse_df = batch_df[batch_df['raw_text'] == \"raw text not present\"]\n",
    "    print(f\"Percentage of data for which parsable raw text is not present: {(len(no_raw_text_parse_df)/len(batch_df)) * 100:.2f}%\")\n",
    "\n",
    "    no_raw_text_df = batch_df[batch_df['raw_text'] == \"\"]\n",
    "    print(f\"Percentage of data for which raw text is not present: {(len(no_raw_text_df)/len(batch_df)) * 100:.2f}%\")\n",
    "\n",
    "    return batch_df[~batch_df['raw_text'].isin(['', \"raw text not present\"])]\n",
    "\n",
    "def save_to_csv(df, writer):\n",
    "    \"\"\"\n",
    "    Save processed data to the CSV file.\n",
    "    \"\"\"\n",
    "    df[['resource_id', 'raw_text', 'languages']].to_csv(writer, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1454c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_raw_df = process_batch(final_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20c35033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454576"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b206e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## save the final dataframe to csv ##############\n",
    "final_raw_df.to_csv(\"./data/extracted_data/curriculum_json_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7d97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_raw_df = pd.read_csv(\"./data/extracted_data/curriculum_json_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236e0048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454576"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd961ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5972774"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_raw_df['resource_id'].iloc[454575]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd14b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = parse_arguments()\n",
    "import os\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "output_path = \"./data/extracted_data/resume_details_cj_17_04_25.csv\"\n",
    "resume_engine = db_connect(\"corpus2_db\")\n",
    "batch_size = 1000\n",
    "offset = 0\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['resource_id', 'raw_text', 'languages'])\n",
    "    # writer.writerow(['resource_id', 'raw_text'])\n",
    "\n",
    "    chunk_size = 1000\n",
    "    num_chunks = len(final_raw_df) // chunk_size + (len(final_raw_df) % chunk_size != 0)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        batch_df = final_raw_df[i*chunk_size : min((i+1)*chunk_size, len(final_raw_df))]\n",
    "\n",
    "        if batch_df.empty:\n",
    "            break\n",
    "        \n",
    "        # output_folder = './data/json_files/'\n",
    "        # save_json_to_files(batch_df, output_folder)\n",
    "        \n",
    "        processed_df = process_batch(batch_df)\n",
    "        save_to_csv(processed_df, csv_file)\n",
    "        \n",
    "        # del batch_df, processed_df\n",
    "        del processed_df\n",
    "        gc.collect()\n",
    "\n",
    "        offset += batch_size\n",
    "        print(f\"Processed {offset} records...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de83b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_raw_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AmitRoy\\miniconda3\\envs\\llm_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AmitRoy\\miniconda3\\envs\\llm_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\AmitRoy\\miniconda3\\envs\\llm_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AmitRoy\\miniconda3\\envs\\llm_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\AmitRoy\\miniconda3\\envs\\llm_env\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied"
     ]
    }
   ],
   "source": [
    "output_raw_df = pd.read_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1d3518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454576"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource_id_list = raw_df['resource_id'].tolist()\n",
    "\n",
    "# query = f\"SELECT * FROM curriculum_data where curriculum_json is not null\"\n",
    "\n",
    "# required_columns = ['resource_id', 'curriculum_name', 'curriculum_data', 'curriculum_json', 'updated_at']\n",
    "\n",
    "# raw_df_2 = pd.read_sql(query, con=resume_engine)\n",
    "# raw_df_2 = raw_df_2[required_columns]\n",
    "\n",
    "# raw_df = raw_df[required_columns]\n",
    "\n",
    "# raw_df_2_filtered = raw_df_2[~raw_df_2['resource_id'].isin(resource_id_list)]\n",
    "\n",
    "# final_raw_df = pd.concat([raw_df, raw_df_2_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2087ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('resume_data_dump.pkl', 'wb') as f:\n",
    "    pickle.dump(final_raw_df, f)\n",
    "\n",
    "with open('resume_data_dump.pkl', 'rb') as f:\n",
    "    final_raw_df = pickle.load(f)\n",
    "\n",
    "\n",
    "def get_json(x):\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except:\n",
    "        return \"Json Load Error\"\n",
    "\n",
    "\n",
    "def get_raw_text(x):\n",
    "    try:\n",
    "        return x['rawText']\n",
    "    except:\n",
    "        return \"Raw Text Not Present\"\n",
    "\n",
    "\n",
    "def get_languages(x):\n",
    "    try:\n",
    "        return x['languages']\n",
    "    except:\n",
    "        return \"Language Not Present\"\n",
    "\n",
    "\n",
    "final_raw_df['json_data'] = final_raw_df['curriculum_json'].apply(lambda x: get_json(x))\n",
    "final_raw_df['raw_text'] = final_raw_df['json_data'].apply(lambda x: get_raw_text(x))\n",
    "final_raw_df['languages'] = final_raw_df['json_data'].apply(lambda x: get_languages(x))\n",
    "\n",
    "final_df_copy = final_raw_df[['resource_id', 'raw_text', 'languages']]\n",
    "\n",
    "final_df_copy.to_excel('resume_data_dump.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
